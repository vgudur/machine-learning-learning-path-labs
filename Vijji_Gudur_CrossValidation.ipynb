{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "17_CrossValidation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or6KGX0nowrg"
      },
      "source": [
        "#Cross Validation\n",
        "\n",
        "<p><a href=\"/guided-machine-learning/\">Back to Index</a></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc58XJsKsYPY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy import stats\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "sns.set()\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "\n",
        "plt.style.use(style='ggplot')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "from scipy.stats import norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb4HCVLg1CiM"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCNChRHgH6MB"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        "def large_print(data, size=140):\n",
        "    display( HTML('<span style=\"font-size:'+ str(size * 2) +'%; line-height:'+str(size)+'%\"><p><p>' + data + '</p></p></span>'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwRS7dOG-Dr2"
      },
      "source": [
        "##### Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNzCnB7S-IF4"
      },
      "source": [
        "## Encoding helper functions\n",
        "def using_cat_encoding(dset, lstvariable):\n",
        "  # fisrt convert the column to category type\n",
        "  for variable in lstvariable:\n",
        "    var_new_col = variable + '_category_type'\n",
        "    var_col_encode = variable + '_encode_val'\n",
        "    dset[var_new_col] =  dset[variable].astype('category')\n",
        "    dset[var_col_encode] = dset[var_new_col].cat.codes\n",
        "    dset.drop(columns=[var_new_col] , inplace=True)\n",
        "  return dset\n",
        "\n",
        "def using_index_encoding(dset, variable):\n",
        "  gle = LabelEncoder()\n",
        "  var_col_encode = variable + '_encode_val'\n",
        "  dset[var_col_encode] = gle.fit_transform(dset[variable])\n",
        "  return dset\n",
        "\n",
        "def using_onehot_encoding (dset , lstvariable):\n",
        "  return pd.get_dummies(dset, columns=lstvariable, prefix=lstvariable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5NvOKrPBLLo"
      },
      "source": [
        "##### Compare the algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjEozs8Oq5P"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint \n",
        "\n",
        "def run_cross_Validation(X,y,  scoring_columns, modeltype='mnb'): \n",
        "  if (modeltype == 'Logistic'):\n",
        "    ## Logistic Regression Model\n",
        "    model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
        "\n",
        "  if (modeltype == 'svm'):\n",
        "    ## Support Vector Machine Model\n",
        "    model = svm.SVC(decision_function_shape=\"ovo\")\n",
        "\n",
        "  if (modeltype == 'rf'):\n",
        "    ##Random Forest Model\n",
        "    model = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\n",
        "\n",
        "  if (modeltype == 'nn'):\n",
        "    ##Neural Network Model\n",
        "    model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(150, 10), random_state=1)\n",
        "  \n",
        "  if (modeltype == 'nb'):\n",
        "    ## Gaussian Naive Bayes\n",
        "    model = GaussianNB()\n",
        "      \n",
        "  if (modeltype == 'mnb'):\n",
        "    ## Multinominal Naive Bayes\n",
        "    model = MultinomialNB()\n",
        "  if (modeltype == 'KNN'):\n",
        "    ## KNN\n",
        "    model = KNeighborsClassifier()\n",
        "  if (modeltype == 'XGB'):\n",
        "    ## XGB\n",
        "    model = XGBClassifier()\n",
        "\n",
        " \n",
        "  ## CV can be integer or a KFold.  Try both\n",
        "  kfold = KFold(n_splits=10, shuffle=True)\n",
        "  cv_results = cross_validate(model, X, y, cv= kfold , scoring=scoring_columns, return_train_score=True)\n",
        "\n",
        "  ##pprint.pprint( \"============CV Results for debugging============\\n\" )\n",
        "  ##pprint.pprint(cv_results )\n",
        "\n",
        "  ##type(cv_results)\n",
        "\n",
        "  ##print ('\\n avg test_accuracy : ', cv_results['test_accuracy'].mean())\n",
        "  return cv_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKAPf77yGiYY"
      },
      "source": [
        "def  Run_Classification (x_tr, y_tr, x_tst, y_tst, feature_cols, usescaling=False, show_roc=False, modeltype='mnb', bDebugPrint=False):\n",
        "\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  from sklearn import svm\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.neural_network import MLPClassifier\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  from sklearn.calibration import calibration_curve\n",
        "  from sklearn.naive_bayes import MultinomialNB\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "  # Create a Logistic Regression Object, perform Logistic Regression\n",
        "  x_tr = pd.DataFrame(x_tr)\n",
        "  y_tr = pd.DataFrame(y_tr)\n",
        "  x_tst = pd.DataFrame(x_tst)\n",
        "  y_tst = pd.DataFrame(y_tst)\n",
        "  \n",
        "  # Scale the data\n",
        "  if (usescaling == True ):\n",
        "    x_tr = ZScore_Standarization_Scaling(x_tr)\n",
        "    x_tst = ZScore_Standarization_Scaling(x_tst)\n",
        "  \n",
        "  if ( bDebugPrint):\n",
        "    print ( \"Running Classifications \")\n",
        "  if (modeltype == 'Logistic'):\n",
        "    ## Logistic Regression Model\n",
        "    model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(x_tr, y_tr)\n",
        "\n",
        "  if (modeltype == 'svm'):\n",
        "    ## Support Vector Machine Model\n",
        "    model = svm.SVC(decision_function_shape=\"ovo\").fit(x_tr, y_tr)\n",
        "\n",
        "  if (modeltype == 'rf'):\n",
        "    ##Random Forest Model\n",
        "    model = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0).fit(x_tr, y_tr)\n",
        "\n",
        "  if (modeltype == 'nn'):\n",
        "    ##Neural Network Model\n",
        "    model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(150, 10), random_state=1).fit(x_tr, y_tr)\n",
        "  \n",
        "  if (modeltype == 'nb'):\n",
        "    ## Gaussian Naive Bayes\n",
        "    model = GaussianNB().fit(x_tr, y_tr)\n",
        "      \n",
        "  if (modeltype == 'mnb'):\n",
        "    ## Multinominal Naive Bayes\n",
        "    model = MultinomialNB().fit(x_tr, y_tr)\n",
        "  if (modeltype == 'KNN'):\n",
        "    ## KNN\n",
        "    model = KNeighborsClassifier().fit(x_tr, y_tr)\n",
        "  if (modeltype == 'XGB'):\n",
        "    ## XGB\n",
        "    model = XGBClassifier().fit(x_tr, y_tr)\n",
        "  tr_score = model.score(x_tr, y_tr)   \n",
        "  tst_score =   model.score(x_tst, y_tst)\n",
        "\n",
        "  y_pred = model.predict(x_tst)\n",
        "  results_model_type, score_train, score_test  = modeltype, tr_score ,   tst_score \n",
        "  if ( bDebugPrint):\n",
        "    large_print ( \"{0}  Model Accuracy for training  :{1}  ,  Testing :{2}\".format( modeltype, tr_score ,   tst_score   ))\n",
        "    large_print('=============show confusion matrix values=============', 60)\n",
        "    # Show the Confusion Matrix\n",
        "    cnf_matrix  = confusion_matrix(y_tst, y_pred)\n",
        "    print ('\\n\\t\\t \\n',cnf_matrix )\n",
        "    large_print('=============Visualize confusion Matrix=============', 60)\n",
        "    #Visualize confusion Matrix  \n",
        "    fig, ax1 = plt.subplots(figsize=(10,5))\n",
        "    tick_marks = np.arange(len(feature_cols))\n",
        "    plt.xticks(tick_marks, feature_cols)\n",
        "    plt.yticks(tick_marks, feature_cols)\n",
        "    # create heatmap\n",
        "    sns.heatmap(pd.DataFrame(cnf_matrix ), cmap=\"YlGnBu\" , annot=True,  fmt='g', ax=ax1)\n",
        "    ax1.xaxis.set_label_position(\"top\")\n",
        "    plt.title('Confusion matrix', y=1.1)\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()    \n",
        "    '''\n",
        "    large_print('=============Confusion Matrix Evaluation Metrics=============', 70)\n",
        "    ## Confusion Matrix Evaluation Metrics \n",
        "    from sklearn import metrics\n",
        "    print(\"\\n\\t\\t Accuracy:\",metrics.accuracy_score(y_tst, y_pred))\n",
        "    print(\"\\n\\t\\t Precision:\",metrics.precision_score(y_tst, y_pred))\n",
        "    print(\"\\n\\t\\t Recall:\",metrics.recall_score(y_tst, y_pred))\n",
        "    '''    \n",
        "    large_print('=============Classification report=============\\n', 70)\n",
        "    print(classification_report(y_tst, y_pred))\n",
        "    if ( show_roc== True):\n",
        "      ## Show ROC Curve \n",
        "      print ( \"Show ROC Curve\")\n",
        "      y_pred_proba = log_reg.predict_proba(x_tst)[::,1]\n",
        "      fpr, tpr, _ = metrics.roc_curve(y_tst,  y_pred_proba)\n",
        "      auc = metrics.roc_auc_score(y_tst, y_pred_proba)\n",
        "      plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "      plt.legend(loc=4)\n",
        "      plt.show()\n",
        "      ## find accuracy\n",
        "      print ('\\n\\t\\t AUC Score  between 0.5 - 1 is perfect classifier. Accuracy of the mdoel is',   ) \n",
        "      ## AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n",
        "  return results_model_type, score_train, score_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdYpMUMc1B5F"
      },
      "source": [
        "def  Run_Plot_Classification ( X_train, y_train, X_test, y_test ):\n",
        "  import numpy as np\n",
        "  np.random.seed(0)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  from sklearn import datasets\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.svm import LinearSVC\n",
        "  from sklearn.calibration import calibration_curve\n",
        "  from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        " ## X, y = datasets.make_classification(n_samples=100000, n_features=20,\n",
        " ##                                     n_informative=2, n_redundant=2)\n",
        "\n",
        "  train_samples = 100  # Samples used for training the models\n",
        "\n",
        "  X_train = pd.DataFrame(X_train)\n",
        "  y_train = pd.DataFrame(y_train)\n",
        "  X_test = pd.DataFrame(X_test)\n",
        "  y_test = pd.DataFrame(y_test)\n",
        "\n",
        "  # Create classifiers\n",
        "  lr = LogisticRegression()\n",
        "  gnb = GaussianNB()\n",
        "  svc = LinearSVC(C=1.0)\n",
        "  rfc = RandomForestClassifier()\n",
        "  nb = MultinomialNB ()\n",
        "\n",
        "\n",
        "  # #############################################################################\n",
        "  # Plot calibration plots\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
        "  ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
        "\n",
        "  ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
        "  for clf, name in [(lr, 'Logistic'),\n",
        "                    (gnb, 'Naive Bayes'),\n",
        "                    (svc, 'Support Vector Classification'),\n",
        "                    (rfc, 'Random Forest')\n",
        "                    ##,                    (nb, 'Multinomial NB')\n",
        "                    ]:\n",
        "      clf.fit(X_train, y_train)\n",
        "      if hasattr(clf, \"predict_proba\"):\n",
        "          prob_pos = clf.predict_proba(X_test)[:, 1]\n",
        "      else:  # use decision function\n",
        "          prob_pos = clf.decision_function(X_test)\n",
        "          prob_pos = \\\n",
        "              (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
        "      fraction_of_positives, mean_predicted_value = \\\n",
        "          calibration_curve(y_test, prob_pos, n_bins=10)\n",
        "\n",
        "      ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
        "              label=\"%s\" % (name, ))\n",
        "\n",
        "      ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
        "              histtype=\"step\", lw=2)\n",
        "\n",
        "  ax1.set_ylabel(\"Fraction of positives\")\n",
        "  ax1.set_ylim([-0.05, 1.05])\n",
        "  ax1.legend(loc=\"lower right\")\n",
        "  ax1.set_title('Calibration plots  (reliability curve)')\n",
        "\n",
        "  ax2.set_xlabel(\"Mean predicted value\")\n",
        "  ax2.set_ylabel(\"Count\")\n",
        "  ax2.legend(loc=\"upper center\", ncol=2)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj8C1dCHD-jC"
      },
      "source": [
        "### PreProcessing_of_Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVnRmDos9olG"
      },
      "source": [
        "def describe_features ( dset, featurelist, NumberValues=True):\n",
        "  ##num_features = dset.select_dtypes([np.number]).columns.tolist()\n",
        "  ##cat_features = dset.select_dtypes( exclude = [np.number] ).columns.tolist()\n",
        "  if NumberValues == True : \n",
        "    astat_df = dset[featurelist].describe().loc[['min','max','mean', 'std']].T.sort_values('max')\n",
        "  else:\n",
        "    astat_df= dset[featurelist].describe()  \n",
        "  return astat_df\n",
        "\n",
        "def find_unique_values_in_Colums ( dset, bprintdebug = False):\n",
        "  if ( bprintdebug == True ):\n",
        "    print('\\nAll Feature Count: ', len(dset.columns))\n",
        "  discrete_feature = [feature for feature in dset.columns \n",
        "                      if len(dset[feature].unique())<50\n",
        "                      and feature not in ['Id']]\n",
        "\n",
        "  if ( bprintdebug == True ):\n",
        "    print('\\nDiscrete Feature Count: ', len(discrete_feature))\n",
        "    for feature in discrete_feature:\n",
        "      print ( '\\nUnique lables for feature :  {0} \\n {1}'.format( feature, dset[feature].unique() )  )\n",
        "  return discrete_feature\n",
        "\n",
        "# What percentage of data is clean?\n",
        "def calculate_missing_values_percent_by_columns(diff):\n",
        "  for col in diff.columns:\n",
        "    pct_missing = np.mean(diff[col].isnull())\n",
        "    pct_missing = round(pct_missing*100, 3)\n",
        "    pct_clean = 100 - pct_missing\n",
        "    \n",
        "    print('{0: >20} - pecent clean: {1: >7}% - percent missing: {2: >7}%'.format(col,pct_clean,  pct_missing))\n",
        "def missing_zero_values_table(df):\n",
        "        zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
        "        mis_val = df.isnull().sum()\n",
        "        mis_val_percent = 100 *( df.isnull().sum() / len(df) )\n",
        "        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n",
        "        mz_table = mz_table.rename(\n",
        "        columns = { 0 : 'Zero Values',1 : 'Missing Values', 2 : '% of Missing Values'})\n",
        "        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n",
        "        mz_table['% Total Zero Missing Values'] = 100 * ( mz_table['Total Zero Missing Values'] / len(df) )\n",
        "        mz_table['Data Type'] = df.dtypes\n",
        "        mz_table['Data Length'] = len(df)\n",
        "        mz_table = mz_table[\n",
        "            mz_table.iloc[:,1] != 0].sort_values(\n",
        "        '% of Missing Values', ascending=False).round(3)\n",
        "\n",
        "        ##print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
        "        ##    \"There are \" + str(mz_table.shape[0]) +\n",
        "         ##     \" columns that have missing values.\")\n",
        "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
        "        return mz_table\n",
        "def  find_Null_Value_Cnt_by_Feature ( dset):\n",
        "  nulls_df = pd.DataFrame(dset.isnull().sum().sort_values(ascending=False)[:25])\n",
        "  nulls_df.columns = ['Null Count']\n",
        "  nulls_df.index.name = 'Feature'\n",
        "  return nulls_df\n",
        "\n",
        "def PreProcessing_Step1 ( dset , bprintdebug = False) :\n",
        " \n",
        "  num_features = dset.select_dtypes([np.number]).columns.tolist()\n",
        "  cat_features = dset.select_dtypes( exclude = [np.number] ).columns.tolist()\n",
        "  if cat_features.count('ID') > 0 :\n",
        "    cat_features.remove('ID')\n",
        "  if cat_features.count('Name') > 0 :\n",
        "    cat_features.remove('Name')\n",
        "  if ( bprintdebug == True ):\n",
        "    print ( '\\nSample data\\n {0} \\n'.format(dset.head(5)) )\n",
        "    print ( \"\\nDataFrame shape: {0}\\n\".format(dset.shape))\n",
        "    print ( \"\\nDataFrame size: {0}\\n\".format(dset.size))\n",
        "    print (\"\\n num_features: {0} \\n cat_features: {1} \\n\".format(num_features,cat_features  ) )\n",
        "    print   (\"\"\"\n",
        "              *********************************\n",
        "              Percent of Cleanness in the data - Method\n",
        "              *********************************\n",
        "             \"\"\")\n",
        "    print ( find_Null_Value_Cnt_by_Feature(dset ) )\n",
        "\n",
        "    print (\"\"\"\n",
        "    *********************************\n",
        "    Data Stats are \n",
        "    *********************************\n",
        "          \"\"\")\n",
        "    print ( \"\\nNumerica feature stats are \\n\")\n",
        "    df_sta1 = describe_features(dset , num_features).sort_values(by='max', ascending=False)\n",
        "    print ( df_sta1 )\n",
        "    print (\"\\nCategorica feature stats are \\n\", describe_features(dset , cat_features, NumberValues=False) )\n",
        "    print ( \"\"\"\n",
        "    *********************************\n",
        "    Finding Unique values in discrete features\n",
        "    *********************************\n",
        "          \"\"\")\n",
        "  discrete_features = find_unique_values_in_Colums ( dset, bprintdebug)\n",
        "  for cval in cat_features:\n",
        "    ##print ( cval)\n",
        "    if  discrete_features.count('cval') <= 0:\n",
        "      cat_features.remove(cval)\n",
        "  return num_features, cat_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPTT6H66zQJf"
      },
      "source": [
        "## HCV Data ( UCI ML Data Repository )\n",
        "\n",
        "Data location : https://archive.ics.uci.edu/ml/machine-learning-databases/00571/hcvdat0.csv\n",
        "\n",
        "      The data set contains laboratory values of blood donors and Hepatitis C patients and demographic values like age.\n",
        "\n",
        "      The target attribute for classification is Category (blood donors vs. Hepatitis C (including its progress ('just' Hepatitis C, Fibrosis, Cirrhosis).\n",
        "\n",
        "\n",
        "      Attribute Information:\n",
        "\n",
        "      All attributes except Category and Sex are numerical. The laboratory data are the attributes 5-14.\n",
        "      1) X (Patient ID/No.)\n",
        "      2) Category (diagnosis) (values: '0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis', '2=Fibrosis', '3=Cirrhosis')\n",
        "      3) Age (in years)\n",
        "      4) Sex (f,m)\n",
        "      5) ALB\n",
        "      6) ALP\n",
        "      7) ALT\n",
        "      8) AST\n",
        "      9) BIL\n",
        "      10) CHE\n",
        "      11) CHOL\n",
        "      12) CREA\n",
        "      13) GGT\n",
        "      14) PROT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZNuuWfPzitL"
      },
      "source": [
        "### Step1 - Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNpKxd2SznEf"
      },
      "source": [
        "db_file_Col_Names = ['Patient_ID', 'Category', 'Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST',\n",
        "       'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n",
        "\n",
        "dataurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00571/hcvdat0.csv'\n",
        "## Read CSV with predefined column list\n",
        "## Skip the headrow\n",
        "## Read CSV without index column\n",
        "df_base = pd.read_csv(dataurl, names = db_file_Col_Names, skiprows=1, na_values='?', index_col=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLv9g4xyzuCa"
      },
      "source": [
        "### Step2 - Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjGaDgbDz0H6",
        "outputId": "151ba901-0097-47a4-921d-ccb3e35a6985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_Features,  cat_Features = PreProcessing_Step1(df_base, bprintdebug=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sample data\n",
            "    Patient_ID       Category  Age Sex  ...    CHOL      CREA      GGT     PROT\n",
            "0           1  0=Blood Donor   32   m  ... 3.23000 106.00000 12.10000 69.00000\n",
            "1           2  0=Blood Donor   32   m  ... 4.80000  74.00000 15.60000 76.50000\n",
            "2           3  0=Blood Donor   32   m  ... 5.20000  86.00000 33.20000 79.30000\n",
            "3           4  0=Blood Donor   32   m  ... 4.74000  80.00000 33.80000 75.70000\n",
            "4           5  0=Blood Donor   32   m  ... 4.32000  76.00000 29.90000 68.70000\n",
            "\n",
            "[5 rows x 14 columns] \n",
            "\n",
            "\n",
            "DataFrame shape: (615, 14)\n",
            "\n",
            "\n",
            "DataFrame size: 8610\n",
            "\n",
            "\n",
            " num_features: ['Patient_ID', 'Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT'] \n",
            " cat_features: ['Category', 'Sex'] \n",
            "\n",
            "\n",
            "              *********************************\n",
            "              Percent of Cleanness in the data - Method\n",
            "              *********************************\n",
            "             \n",
            "            Null Count\n",
            "Feature               \n",
            "ALP                 18\n",
            "CHOL                10\n",
            "PROT                 1\n",
            "ALT                  1\n",
            "ALB                  1\n",
            "GGT                  0\n",
            "CREA                 0\n",
            "CHE                  0\n",
            "BIL                  0\n",
            "AST                  0\n",
            "Sex                  0\n",
            "Age                  0\n",
            "Category             0\n",
            "Patient_ID           0\n",
            "\n",
            "    *********************************\n",
            "    Data Stats are \n",
            "    *********************************\n",
            "          \n",
            "\n",
            "Numerica feature stats are \n",
            "\n",
            "                min        max      mean       std\n",
            "CREA        8.00000 1079.10000  81.28780  49.75617\n",
            "GGT         4.50000  650.90000  39.53317  54.66107\n",
            "Patient_ID  1.00000  615.00000 308.00000 177.67949\n",
            "ALP        11.30000  416.60000  68.28392  26.02832\n",
            "ALT         0.90000  325.30000  28.45081  25.46969\n",
            "AST        10.60000  324.00000  34.78634  33.09069\n",
            "BIL         0.80000  254.00000  11.39675  19.67315\n",
            "PROT       44.80000   90.00000  72.04414   5.40264\n",
            "ALB        14.90000   82.20000  41.62020   5.78063\n",
            "Age        19.00000   77.00000  47.40813  10.05511\n",
            "CHE         1.42000   16.41000   8.19663   2.20566\n",
            "CHOL        1.43000    9.67000   5.36810   1.13273\n",
            "\n",
            "Categorica feature stats are \n",
            "              Category  Sex\n",
            "count             615  615\n",
            "unique              5    2\n",
            "top     0=Blood Donor    m\n",
            "freq              533  377\n",
            "\n",
            "    *********************************\n",
            "    Finding Unique values in discrete features\n",
            "    *********************************\n",
            "          \n",
            "\n",
            "All Feature Count:  14\n",
            "\n",
            "Discrete Feature Count:  3\n",
            "\n",
            "Unique lables for feature :  Category \n",
            " ['0=Blood Donor' '0s=suspect Blood Donor' '1=Hepatitis' '2=Fibrosis'\n",
            " '3=Cirrhosis']\n",
            "\n",
            "Unique lables for feature :  Age \n",
            " [32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55\n",
            " 56 57 58 59 60 61 62 63 64 65 66 67 68 70 71 76 77 74 19 23 25 27 29 30\n",
            " 75]\n",
            "\n",
            "Unique lables for feature :  Sex \n",
            " ['m' 'f']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5JLvdJV0aAE",
        "outputId": "fa7f7a15-075d-49ed-ef15-b21f1f34ce3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Find unique values in Category columns  Category , Sex \n",
        "print ( \"Unique values in Sex feature are \" , df_base['Sex'].unique() )\n",
        "print ( \"Unique values in Category feature are \" , df_base['Category'].unique()  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique values in Sex feature are  ['m' 'f']\n",
            "Unique values in Category feature are  ['0=Blood Donor' '0s=suspect Blood Donor' '1=Hepatitis' '2=Fibrosis'\n",
            " '3=Cirrhosis']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PyUZWTx-uj5"
      },
      "source": [
        "### Drop missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0K0vRC3-2wC",
        "outputId": "a881806f-1f39-4dde-9a7b-2e2341550d71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## drop missing records from both datasets\n",
        "# making new data frame with dropped NA values  \n",
        "##df_train  =  pd.DataFrame(df_census_train, columns= x_cols + y_cols  + ['income'] ) \n",
        "df_HCV = df_base.dropna(axis = 0, how ='any')\n",
        "\n",
        "\n",
        "print(\"\\nCleaning NA values in training set \\n\\tLength before: {0} Length After : {1}\".format( len(df_base), len(df_HCV)    )    ) \n",
        "print(\"\\n\\tNumber of rows with at least 1 NA value: \", ( len(df_base) - len(df_HCV)  )) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cleaning NA values in training set \n",
            "\tLength before: 615 Length After : 589\n",
            "\n",
            "\tNumber of rows with at least 1 NA value:  26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDx_2W3N2GCH"
      },
      "source": [
        "###Step3 - Encoding\n",
        "\n",
        "We need to encode  these category values to numerical values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygyAD7ai2UPJ",
        "outputId": "1e3973c1-cb44-4b70-9fa6-d0752c0fae4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Encoding above ordinal data using OrdinalEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinalencoder = OrdinalEncoder()\n",
        "'''\n",
        "df_HCV['Gender_index'] = ordinalencoder.fit_transform(df_HCV[['Sex']])\n",
        "\n",
        "'''\n",
        "\n",
        "df_HCV[\"Gender_index\"] = df_HCV[\"Sex\"].apply(lambda x:0 if x.strip() =='m' else 1)\n",
        "print ( \"Unique values in Gender_index feature are \" , df_HCV['Gender_index'].unique() )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique values in Gender_index feature are  [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YweBkd03JOy",
        "outputId": "14a16ec7-83be-4357-f526-3bbab6fc7cdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Encode Category target column \n",
        "df_HCV[['Category_index','Category_Name']] = df_HCV.Category.str.split(\"=\",expand=True) \n",
        "print ( \"Unique values in Category_index feature are \" , df_HCV['Category_index'].unique() )\n",
        "df_HCV['Category_index'] = LabelEncoder().fit_transform(df_HCV[['Category_index']])\n",
        "print ( \"Unique values in Category_index feature are \" , df_HCV['Category_index'].unique() )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique values in Category_index feature are  ['0' '0s' '1' '2' '3']\n",
            "Unique values in Category_index feature are  [0 1 2 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYdIlMVM5-eC"
      },
      "source": [
        "### Step4 - Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SbPQv6h21Ss",
        "outputId": "8ef3f84f-29e1-46e0-e75f-abf62f3dae15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##df_HCV.head() \n",
        "\n",
        "num_features = df_HCV.select_dtypes([np.number]).columns.tolist()\n",
        "#defin features: text and predict output: Category\n",
        "feature_labels = num_features\n",
        "\n",
        "## drop Patient_ID and target variable from feature_lables\n",
        "if ( feature_labels.count('Patient_ID') >0  ):\n",
        "  feature_labels.remove('Patient_ID')\n",
        "if ( feature_labels.count('Category_index') >0  ):\n",
        "  feature_labels.remove('Category_index')\n",
        "target_variable = ['Category_index']\n",
        "\n",
        "\n",
        "##df_filtered = pd.DataFrame ( df_HCV, columns=list(feature_labels))\n",
        "##df_New = pd.DataFrame ( dset, columns=list(num_features))\n",
        "\n",
        "X = pd.DataFrame ( df_HCV, columns=list(feature_labels))\n",
        "y = df_HCV['Category_index']\n",
        "X, y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     Age      ALB       ALP      ALT  ...      CREA       GGT     PROT  Gender_index\n",
              " 0     32 38.50000  52.50000  7.70000  ... 106.00000  12.10000 69.00000             0\n",
              " 1     32 38.50000  70.30000 18.00000  ...  74.00000  15.60000 76.50000             0\n",
              " 2     32 46.90000  74.70000 36.20000  ...  86.00000  33.20000 79.30000             0\n",
              " 3     32 43.20000  52.00000 30.60000  ...  80.00000  33.80000 75.70000             0\n",
              " 4     32 39.20000  74.10000 32.60000  ...  76.00000  29.90000 68.70000             0\n",
              " ..   ...      ...       ...      ...  ...       ...       ...      ...           ...\n",
              " 608   58 34.00000  46.40000 15.00000  ...  56.00000  49.70000 80.60000             1\n",
              " 609   59 39.00000  51.30000 19.60000  ... 136.10000 101.10000 70.50000             1\n",
              " 610   62 32.00000 416.60000  5.90000  ...  55.70000 650.90000 68.50000             1\n",
              " 611   64 24.00000 102.80000  2.90000  ...  63.00000  35.90000 71.30000             1\n",
              " 612   64 29.00000  87.30000  3.50000  ...  66.70000  64.20000 82.00000             1\n",
              " \n",
              " [589 rows x 12 columns], 0      0\n",
              " 1      0\n",
              " 2      0\n",
              " 3      0\n",
              " 4      0\n",
              "       ..\n",
              " 608    4\n",
              " 609    4\n",
              " 610    4\n",
              " 611    4\n",
              " 612    4\n",
              " Name: Category_index, Length: 589, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmC5hJ2ME9C3"
      },
      "source": [
        "### Step5 - Run Classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4WDG1kb834n",
        "outputId": "ae743e5d-0508-4d65-8d70-12adfa170291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print ( \"features {0} \\ntarget : {1}\".format( feature_labels, target_variable))\n",
        "\n",
        "x_train , x_test , y_train , y_test = train_test_split(X , y, test_size = 0.20, random_state = 42)\n",
        "print( \"Size of \\nx_train :{}  y_train :{} \\nx_test :{}  y_test :{}\".format(  x_train.shape ,  y_train.shape , x_test.shape, y_test.shape )  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features ['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT', 'Gender_index'] \n",
            "target : ['Category_index']\n",
            "Size of \n",
            "x_train :(471, 12)  y_train :(471,) \n",
            "x_test :(118, 12)  y_test :(118,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-85UtedICt"
      },
      "source": [
        "##### Find score by multiple ML Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxK4dV6EVWpa",
        "outputId": "007de65c-ecba-47d7-e2a1-1934b718461b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "models = ['mnb', 'Logistic' , 'svm', 'rf' , 'nn', 'nb','KNN','XGB']\n",
        "model_results = []\n",
        "for model_name in models:\n",
        "  mdl_results =  Run_Classification (x_train , y_train , x_test , y_test ,  X.columns, modeltype= model_name   )\n",
        "  model_results.append(mdl_results)\n",
        "##print ( model_results)\n",
        "df = pd.DataFrame(model_results, columns =['Algorithm', 'Training_Score', 'Testing_Score' ])   \n",
        "print (\"================= Scores of Algorithms ===================== \\n\\n\")\n",
        "print(df)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Algorithm  Training_Score  Testing_Score\n",
            "0       mnb         0.94268        0.86441\n",
            "1  Logistic         0.98089        0.92373\n",
            "2       svm         0.95117        0.88136\n",
            "3        rf         1.00000        0.88983\n",
            "4        nn         1.00000        0.91525\n",
            "5        nb         0.95117        0.88136\n",
            "6       KNN         0.95329        0.89831\n",
            "7       XGB         1.00000        0.92373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pmMkLe1nnvO"
      },
      "source": [
        "#### Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyD0ea-SSK8P"
      },
      "source": [
        " ## We can specify more than one metrics\n",
        "scoring_metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
        "models = ['Logistic',  'svm', 'rf' , 'nn', 'nb', 'mnb', 'KNN','XGB']\n",
        "cv_results = []\n",
        "cv_stats  = []\n",
        "for model_name in models:\n",
        "  cv =  run_cross_Validation (X,y, scoring_metrics, model_name)\n",
        "  cv_results.append(cv)\n",
        "  cv_stats.append( [model_name \n",
        "                    , cv['test_accuracy'].mean(),  cv['train_accuracy'].mean()  \n",
        "                    , cv['test_precision_weighted'].mean(),  cv['train_precision_weighted'].mean()  \n",
        "                    , cv['test_recall_weighted'].mean(),  cv['train_recall_weighted'].mean()  \n",
        "                    , cv['test_f1_weighted'].mean(),  cv['train_f1_weighted'].mean()  \n",
        "                    , cv['score_time'].mean(),  cv['fit_time'].mean()\n",
        "                                        ]  \n",
        "                   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_dffo4rtiiq",
        "outputId": "28dbc85f-a29e-45e2-8d94-ee2c9619b18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "print (\"================= Cross validation of Algorithms - Stats ===================== \\n\\n\")\n",
        "df_cv = pd.DataFrame(cv_stats, \n",
        "                     columns = ['ModelName' , 'tst_Accuracy' , 'train_Accuracy',  'tst_pw' , 'train_pw' , 'tst_rw', 'train_rw' , 'tst_fw', 'train_fw' , 'score_time' , 'fit_time']\n",
        "                     , index=None)\n",
        "df_cv.head(20) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================= Cross validation of Algorithms - Stats ===================== \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ModelName</th>\n",
              "      <th>tst_Accuracy</th>\n",
              "      <th>train_Accuracy</th>\n",
              "      <th>tst_pw</th>\n",
              "      <th>train_pw</th>\n",
              "      <th>tst_rw</th>\n",
              "      <th>train_rw</th>\n",
              "      <th>tst_fw</th>\n",
              "      <th>train_fw</th>\n",
              "      <th>score_time</th>\n",
              "      <th>fit_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic</td>\n",
              "      <td>0.94231</td>\n",
              "      <td>0.97698</td>\n",
              "      <td>0.95408</td>\n",
              "      <td>0.97647</td>\n",
              "      <td>0.94231</td>\n",
              "      <td>0.97698</td>\n",
              "      <td>0.94177</td>\n",
              "      <td>0.97646</td>\n",
              "      <td>0.00459</td>\n",
              "      <td>0.05641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>svm</td>\n",
              "      <td>0.93220</td>\n",
              "      <td>0.94775</td>\n",
              "      <td>0.90416</td>\n",
              "      <td>0.92982</td>\n",
              "      <td>0.93220</td>\n",
              "      <td>0.94775</td>\n",
              "      <td>0.91632</td>\n",
              "      <td>0.93671</td>\n",
              "      <td>0.00405</td>\n",
              "      <td>0.00595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rf</td>\n",
              "      <td>0.94404</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.92259</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.94404</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.93152</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.07859</td>\n",
              "      <td>1.79723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nn</td>\n",
              "      <td>0.94056</td>\n",
              "      <td>0.99660</td>\n",
              "      <td>0.94774</td>\n",
              "      <td>0.99723</td>\n",
              "      <td>0.94056</td>\n",
              "      <td>0.99660</td>\n",
              "      <td>0.93943</td>\n",
              "      <td>0.99676</td>\n",
              "      <td>0.00573</td>\n",
              "      <td>1.03421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nb</td>\n",
              "      <td>0.92709</td>\n",
              "      <td>0.93869</td>\n",
              "      <td>0.93525</td>\n",
              "      <td>0.94160</td>\n",
              "      <td>0.92709</td>\n",
              "      <td>0.93869</td>\n",
              "      <td>0.92935</td>\n",
              "      <td>0.93877</td>\n",
              "      <td>0.00469</td>\n",
              "      <td>0.00298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>mnb</td>\n",
              "      <td>0.91517</td>\n",
              "      <td>0.92153</td>\n",
              "      <td>0.93016</td>\n",
              "      <td>0.94169</td>\n",
              "      <td>0.91517</td>\n",
              "      <td>0.92153</td>\n",
              "      <td>0.91944</td>\n",
              "      <td>0.92754</td>\n",
              "      <td>0.00329</td>\n",
              "      <td>0.00215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.92361</td>\n",
              "      <td>0.94605</td>\n",
              "      <td>0.90473</td>\n",
              "      <td>0.93812</td>\n",
              "      <td>0.92361</td>\n",
              "      <td>0.94605</td>\n",
              "      <td>0.90887</td>\n",
              "      <td>0.93777</td>\n",
              "      <td>0.00618</td>\n",
              "      <td>0.00243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGB</td>\n",
              "      <td>0.94912</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.94309</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.94912</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.94170</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00468</td>\n",
              "      <td>0.18836</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ModelName  tst_Accuracy  train_Accuracy  ...  train_fw  score_time  fit_time\n",
              "0  Logistic       0.94231         0.97698  ...   0.97646     0.00459   0.05641\n",
              "1       svm       0.93220         0.94775  ...   0.93671     0.00405   0.00595\n",
              "2        rf       0.94404         1.00000  ...   1.00000     0.07859   1.79723\n",
              "3        nn       0.94056         0.99660  ...   0.99676     0.00573   1.03421\n",
              "4        nb       0.92709         0.93869  ...   0.93877     0.00469   0.00298\n",
              "5       mnb       0.91517         0.92153  ...   0.92754     0.00329   0.00215\n",
              "6       KNN       0.92361         0.94605  ...   0.93777     0.00618   0.00243\n",
              "7       XGB       0.94912         1.00000  ...   1.00000     0.00468   0.18836\n",
              "\n",
              "[8 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    }
  ]
}