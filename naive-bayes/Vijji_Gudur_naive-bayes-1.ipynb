{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "naive-bayes-1-spam.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgudur/machine-learning-learning-path-labs/blob/patch-3/naive-bayes/Vijji_Gudur_naive-bayes-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwh5DX_JhynO"
      },
      "source": [
        "# Naive Bayes Spam Filtering\n",
        "\n",
        "### Overview\n",
        "\n",
        "We all hate spam, so developing a classifier to classify email as spam or not spam is useful.  \n",
        "\n",
        "### Builds on\n",
        "None\n",
        "\n",
        "### Run time\n",
        "approx. 20-30 minutes\n",
        "\n",
        "### Notes\n",
        "\n",
        "We can do Naive Bayes classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3NMFpI2hynO"
      },
      "source": [
        "## Step 1: Load Data\n",
        "We will load the dataframe into pandas.  Since the outcome label is \"ham\" or \"spam\", that will be our label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0m1L6YNhynP"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "data_location = 'SMSSpamCollection.txt'\n",
        "data_url = 'https://elephantscale-public.s3.amazonaws.com/data/spam/SMSSpamCollection.txt'\n",
        "\n",
        "if not os.path.exists(data_location):\n",
        "    urllib.request.urlretrieve(data_url, data_location)\n",
        "    print (\"Downloading  \", data_url)\n",
        "print ('data_location: ', data_location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-S19ahUhynS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(data_location, sep='\\t')\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIihLUZ7hynV"
      },
      "source": [
        "## Step 2 - Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M77-pVREhynV"
      },
      "source": [
        "## TODO :  Count spam/ham\n",
        "## Hint : group by 'isspam'\n",
        "## Question : Is there a data skew?\n",
        "dataset.groupby(\"???\").size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNfyJuVqhynY"
      },
      "source": [
        "## Step 3: Vectorize Using TF/IDF\n",
        "\n",
        "Let's use tf/idf for vecorization at first.  TF/IDF will take and count the instances of each term, and then divide by the total frequecy of that term in the entire dataset.  \n",
        "\n",
        "This leads to very highly dimensional data, because every word in the document will lead to a dimension in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDXpST_GhynY"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "pipeline = Pipeline([('vec', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer())])\n",
        "\n",
        "x = pipeline.fit_transform(dataset['text'])\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErZ7_BMJhynb"
      },
      "source": [
        "## we can leave y as 'spam' / 'ham'\n",
        "# y = dataset['isspam']\n",
        "\n",
        "## TODO : encode y\n",
        "## Hint : use the 'isspam' column\n",
        "y = pd.factorize(dataset['???'])[0]\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAFsBB53hynd"
      },
      "source": [
        "## Step 4: Split train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmEoy9j_hyne"
      },
      "source": [
        "## TODO: Use training / test split of 80%/20%\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train, y_test = train_test_split(x,y,  test_size=???)\n",
        "## to control train/test split set random_state to a number\n",
        "# x_train,x_test,y_train, y_test = train_test_split(x,y, random_state=0, test_size=0.3)\n",
        "\n",
        "print (\"x_train :\" , x_train.shape )\n",
        "print (\"x_test :\", x_test.shape)\n",
        "print (\"y_train :\", y_train.shape)\n",
        "print (\"y_test :\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoITmZS8hynh"
      },
      "source": [
        "## Step 5 : Run Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTl1eNOShynh"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB ()\n",
        "\n",
        "##TODO : fit on (x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4VSpKe1hynj"
      },
      "source": [
        "## TODO : Train and Evaluate the Model\n",
        "\n",
        "- Compute training / testing accuracy\n",
        "- compute confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7VabfAVhynk"
      },
      "source": [
        "## Run your own data!\n",
        "\n",
        "Now it's your turn!   Make a new dataframe with some sample test data of your own creation.  Make some \"spammy\" SMSes and some ordinary ones.  See how our spam filter does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm1cDBmvhynk"
      },
      "source": [
        "mydata = pd.DataFrame ( { 'text' : [\n",
        "                                     'can we meet for lunch?',\n",
        "                                     'win win win instant tickets!',\n",
        "                                     'ultra cheap medications!!!',\n",
        "                                     'your text here'\n",
        "]})\n",
        "\n",
        "mydata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-OGUBBhynn"
      },
      "source": [
        "my_x = pipeline.transform(mydata['text'])\n",
        "my_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhutGQahynq"
      },
      "source": [
        "my_pred = model.predict(my_x)\n",
        "my_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcdyejFrhyns"
      },
      "source": [
        "mydata['prediction'] = my_pred\n",
        "mydata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R1Y1FhNhynv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}